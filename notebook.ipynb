{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c79cbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sami.saily/work/frends-llm/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from unsloth_mlx import (\n",
    "    FastLanguageModel,\n",
    "    SFTTrainer,\n",
    "    TrainingArguments,    \n",
    "    prepare_dataset,    \n",
    "    get_chat_template,\n",
    "    format_chat_template,\n",
    "    create_training_data,\n",
    "    save_model_hf_format,\n",
    "    export_to_gguf,\n",
    "    get_training_config,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "135d4fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 133152.51it/s]\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 7096\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42655181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA configuration set: rank=16, alpha=16, modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], dropout=0\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acd14d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.2\",\n",
    ")\n",
    "\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d72fbe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    Split,\n",
    ")\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"datasets/frends-bpmn-dataset.jsonl\", split=Split.TRAIN)\n",
    "#dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "dataset_val = load_dataset(\"json\", data_files=\"datasets/frends-bpmn-dataset-val.jsonl\")\n",
    "#dataset_val = dataset_val.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8a2fcab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Create a Frends process that assigns a variable and returns it as output.',\n",
       " 'input': '',\n",
       " 'output': 'Here\\'s a simple Frends process definition that assigns a variable and returns it:\\n\\n```json\\n{\\n \"Processes\": [{\\n \"Name\": \"Fine Tuning: simple\",\\n \"Version\": 1,\\n \"ElementParameters\": [\\n {\\n \"Id\": \"StartEvent_1\",\\n \"Type\": 0,\\n \"SelectedTypeId\": \"ManualTrigger\",\\n \"Name\": \"Manual Trigger\"\\n },\\n {\\n \"Id\": \"Activity_0zi7vwm\",\\n \"Type\": 12,\\n \"Parameters\": {\\n \"variableName\": \"result\",\\n \"variableExpression\": {\\n \"mode\": \"text\",\\n \"value\": \"ok\"\\n },\\n \"shouldAssignVariable\": {\\n \"mode\": \"toggle\",\\n \"value\": true\\n }\\n },\\n \"Name\": \"Assign result\"\\n },\\n {\\n \"Id\": \"Event_19glogw\",\\n \"Type\": 5,\\n \"Parameters\": {\\n \"expression\": {\\n \"mode\": \"csharp\",\\n \"value\": \"#var.result\"\\n }\\n }\\n }\\n ],\\n \"TriggersJson\": \"[{\\\\\"$type\\\\\":\\\\\"ManualTrigger\\\\\",\\\\\"name\\\\\":\\\\\"Manual Trigger\\\\\",\\\\\"id\\\\\":\\\\\"StartEvent_1\\\\\"}]\"\\n }]\\n}```\\n\\nThis process uses a Manual Trigger to start, assigns the variable \\'result\\' with value \\'ok\\', and returns it at the end event.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f3039e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized:\n",
      "  Output dir: outputs\n",
      "  Adapter path: outputs/adapters\n",
      "  Learning rate: 0.0002\n",
      "  Iterations: 60\n",
      "  Batch size: 2\n",
      "  LoRA r=16, alpha=16\n",
      "  Native training: True\n",
      "  LR scheduler: linear\n",
      "  Grad checkpoint: False\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    val_dataset = dataset_val,\n",
    "    #dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,        \n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6ff141e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mx.metal.device_info is deprecated and will be removed in a future version. Use mx.device_info instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Starting Fine-Tuning\n",
      "======================================================================\n",
      "\n",
      "[Using Native MLX Training]\n",
      "\n",
      "Applying LoRA adapters...\n",
      "Applying LoRA to 16 layers: {'rank': 16, 'scale': 1.0, 'dropout': 0, 'keys': ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']}\n",
      "✓ LoRA applied successfully to 16 layers\n",
      "  Trainable LoRA parameters: 224\n",
      "Preparing training data...\n",
      "  Detected format: alpaca\n",
      "✓ Prepared 4 training samples\n",
      "  Saved to: outputs/train.jsonl\n",
      "✓ Created validation set (copied from train)\n",
      "\n",
      "Training configuration:\n",
      "  Iterations: 60\n",
      "  Batch size: 2\n",
      "  Learning rate: 0.0002\n",
      "  LR scheduler: linear\n",
      "  Grad checkpoint: True\n",
      "  Adapter file: outputs/adapters/adapters.safetensors\n",
      "\n",
      "Loaded 4 training samples, 4 validation samples\n",
      "Starting training loop...\n",
      "Starting training..., iters: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 2/2 [00:00<00:00,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: Val loss 2.631, Val took 0.997s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: Train loss 2.803, Learning Rate 2.000e-04, It/sec 0.698, Tokens/sec 532.861, Trained Tokens 763, Peak mem 2.618 GB\n",
      "Iter 2: Train loss 2.203, Learning Rate 1.967e-04, It/sec 0.725, Tokens/sec 497.095, Trained Tokens 1449, Peak mem 2.618 GB\n",
      "Iter 3: Train loss 1.703, Learning Rate 1.933e-04, It/sec 0.790, Tokens/sec 541.727, Trained Tokens 2135, Peak mem 2.618 GB\n",
      "Iter 4: Train loss 1.518, Learning Rate 1.900e-04, It/sec 0.720, Tokens/sec 549.054, Trained Tokens 2898, Peak mem 2.708 GB\n",
      "Iter 5: Train loss 0.990, Learning Rate 1.867e-04, It/sec 0.740, Tokens/sec 564.480, Trained Tokens 3661, Peak mem 2.708 GB\n",
      "Iter 6: Train loss 0.697, Learning Rate 1.833e-04, It/sec 0.795, Tokens/sec 545.583, Trained Tokens 4347, Peak mem 2.708 GB\n",
      "Iter 7: Train loss 0.550, Learning Rate 1.800e-04, It/sec 0.735, Tokens/sec 561.128, Trained Tokens 5110, Peak mem 2.708 GB\n",
      "Iter 8: Train loss 0.289, Learning Rate 1.767e-04, It/sec 0.798, Tokens/sec 547.724, Trained Tokens 5796, Peak mem 2.708 GB\n",
      "Iter 9: Train loss 0.149, Learning Rate 1.733e-04, It/sec 0.795, Tokens/sec 545.546, Trained Tokens 6482, Peak mem 2.708 GB\n",
      "Iter 10: Train loss 0.287, Learning Rate 1.700e-04, It/sec 0.736, Tokens/sec 561.780, Trained Tokens 7245, Peak mem 2.708 GB\n",
      "Iter 11: Train loss 0.105, Learning Rate 1.667e-04, It/sec 0.726, Tokens/sec 553.958, Trained Tokens 8008, Peak mem 2.708 GB\n",
      "Iter 12: Train loss 0.124, Learning Rate 1.633e-04, It/sec 0.785, Tokens/sec 538.290, Trained Tokens 8694, Peak mem 2.708 GB\n",
      "Iter 13: Train loss 0.081, Learning Rate 1.600e-04, It/sec 0.725, Tokens/sec 553.220, Trained Tokens 9457, Peak mem 2.708 GB\n",
      "Iter 14: Train loss 0.087, Learning Rate 1.567e-04, It/sec 0.775, Tokens/sec 531.965, Trained Tokens 10143, Peak mem 2.708 GB\n",
      "Iter 15: Train loss 0.050, Learning Rate 1.533e-04, It/sec 0.727, Tokens/sec 554.862, Trained Tokens 10906, Peak mem 2.708 GB\n",
      "Iter 16: Train loss 0.056, Learning Rate 1.500e-04, It/sec 0.782, Tokens/sec 536.659, Trained Tokens 11592, Peak mem 2.708 GB\n",
      "Iter 17: Train loss 0.055, Learning Rate 1.467e-04, It/sec 0.726, Tokens/sec 554.309, Trained Tokens 12355, Peak mem 2.708 GB\n",
      "Iter 18: Train loss 0.034, Learning Rate 1.433e-04, It/sec 0.787, Tokens/sec 539.638, Trained Tokens 13041, Peak mem 2.708 GB\n",
      "Iter 19: Train loss 0.030, Learning Rate 1.400e-04, It/sec 0.772, Tokens/sec 529.276, Trained Tokens 13727, Peak mem 2.708 GB\n",
      "Iter 20: Train loss 0.045, Learning Rate 1.367e-04, It/sec 0.722, Tokens/sec 550.759, Trained Tokens 14490, Peak mem 2.708 GB\n",
      "Iter 21: Train loss 0.026, Learning Rate 1.333e-04, It/sec 0.761, Tokens/sec 522.023, Trained Tokens 15176, Peak mem 2.708 GB\n",
      "Iter 22: Train loss 0.029, Learning Rate 1.300e-04, It/sec 0.712, Tokens/sec 543.401, Trained Tokens 15939, Peak mem 2.708 GB\n",
      "Iter 23: Train loss 0.020, Learning Rate 1.267e-04, It/sec 0.793, Tokens/sec 543.693, Trained Tokens 16625, Peak mem 2.708 GB\n",
      "Iter 24: Train loss 0.023, Learning Rate 1.233e-04, It/sec 0.701, Tokens/sec 535.173, Trained Tokens 17388, Peak mem 2.708 GB\n",
      "Iter 25: Train loss 0.015, Learning Rate 1.200e-04, It/sec 0.737, Tokens/sec 562.511, Trained Tokens 18151, Peak mem 2.708 GB\n",
      "Iter 26: Train loss 0.009, Learning Rate 1.167e-04, It/sec 0.782, Tokens/sec 536.138, Trained Tokens 18837, Peak mem 2.708 GB\n",
      "Iter 27: Train loss 0.007, Learning Rate 1.133e-04, It/sec 0.792, Tokens/sec 543.465, Trained Tokens 19523, Peak mem 2.708 GB\n",
      "Iter 28: Train loss 0.006, Learning Rate 1.100e-04, It/sec 0.736, Tokens/sec 561.191, Trained Tokens 20286, Peak mem 2.708 GB\n",
      "Iter 29: Train loss 0.006, Learning Rate 1.067e-04, It/sec 0.798, Tokens/sec 547.219, Trained Tokens 20972, Peak mem 2.708 GB\n",
      "Iter 30: Train loss 0.005, Learning Rate 1.033e-04, It/sec 0.734, Tokens/sec 559.893, Trained Tokens 21735, Peak mem 2.708 GB\n",
      "Iter 31: Train loss 0.004, Learning Rate 1.000e-04, It/sec 0.736, Tokens/sec 561.236, Trained Tokens 22498, Peak mem 2.708 GB\n",
      "Iter 32: Train loss 0.006, Learning Rate 9.667e-05, It/sec 0.795, Tokens/sec 545.510, Trained Tokens 23184, Peak mem 2.708 GB\n",
      "Iter 33: Train loss 0.006, Learning Rate 9.333e-05, It/sec 0.798, Tokens/sec 547.405, Trained Tokens 23870, Peak mem 2.708 GB\n",
      "Iter 34: Train loss 0.005, Learning Rate 9.000e-05, It/sec 0.723, Tokens/sec 551.854, Trained Tokens 24633, Peak mem 2.708 GB\n",
      "Iter 35: Train loss 0.005, Learning Rate 8.667e-05, It/sec 0.723, Tokens/sec 551.987, Trained Tokens 25396, Peak mem 2.708 GB\n",
      "Iter 36: Train loss 0.005, Learning Rate 8.333e-05, It/sec 0.779, Tokens/sec 534.148, Trained Tokens 26082, Peak mem 2.708 GB\n",
      "Iter 37: Train loss 0.004, Learning Rate 8.000e-05, It/sec 0.724, Tokens/sec 552.239, Trained Tokens 26845, Peak mem 2.708 GB\n",
      "Iter 38: Train loss 0.005, Learning Rate 7.667e-05, It/sec 0.787, Tokens/sec 539.880, Trained Tokens 27531, Peak mem 2.708 GB\n",
      "Iter 39: Train loss 0.004, Learning Rate 7.333e-05, It/sec 0.718, Tokens/sec 547.488, Trained Tokens 28294, Peak mem 2.708 GB\n",
      "Iter 40: Train loss 0.004, Learning Rate 7.000e-05, It/sec 0.776, Tokens/sec 532.190, Trained Tokens 28980, Peak mem 2.708 GB\n",
      "Iter 41: Train loss 0.004, Learning Rate 6.667e-05, It/sec 0.720, Tokens/sec 549.065, Trained Tokens 29743, Peak mem 2.708 GB\n",
      "Iter 42: Train loss 0.004, Learning Rate 6.333e-05, It/sec 0.778, Tokens/sec 533.678, Trained Tokens 30429, Peak mem 2.708 GB\n",
      "Iter 43: Train loss 0.004, Learning Rate 6.000e-05, It/sec 0.781, Tokens/sec 536.024, Trained Tokens 31115, Peak mem 2.708 GB\n",
      "Iter 44: Train loss 0.005, Learning Rate 5.667e-05, It/sec 0.723, Tokens/sec 551.524, Trained Tokens 31878, Peak mem 2.708 GB\n",
      "Iter 45: Train loss 0.004, Learning Rate 5.333e-05, It/sec 0.775, Tokens/sec 531.797, Trained Tokens 32564, Peak mem 2.708 GB\n",
      "Iter 46: Train loss 0.005, Learning Rate 5.000e-05, It/sec 0.726, Tokens/sec 553.847, Trained Tokens 33327, Peak mem 2.708 GB\n",
      "Iter 47: Train loss 0.003, Learning Rate 4.667e-05, It/sec 0.778, Tokens/sec 533.599, Trained Tokens 34013, Peak mem 2.708 GB\n",
      "Iter 48: Train loss 0.005, Learning Rate 4.333e-05, It/sec 0.724, Tokens/sec 552.717, Trained Tokens 34776, Peak mem 2.708 GB\n",
      "Iter 49: Train loss 0.005, Learning Rate 4.000e-05, It/sec 0.722, Tokens/sec 550.516, Trained Tokens 35539, Peak mem 2.708 GB\n",
      "Iter 50: Train loss 0.004, Learning Rate 3.667e-05, It/sec 0.778, Tokens/sec 533.388, Trained Tokens 36225, Peak mem 2.708 GB\n",
      "Iter 51: Train loss 0.004, Learning Rate 3.333e-05, It/sec 0.781, Tokens/sec 535.493, Trained Tokens 36911, Peak mem 2.708 GB\n",
      "Iter 52: Train loss 0.004, Learning Rate 3.000e-05, It/sec 0.723, Tokens/sec 551.994, Trained Tokens 37674, Peak mem 2.708 GB\n",
      "Iter 53: Train loss 0.004, Learning Rate 2.667e-05, It/sec 0.722, Tokens/sec 550.542, Trained Tokens 38437, Peak mem 2.708 GB\n",
      "Iter 54: Train loss 0.004, Learning Rate 2.333e-05, It/sec 0.783, Tokens/sec 537.343, Trained Tokens 39123, Peak mem 2.708 GB\n",
      "Iter 55: Train loss 0.004, Learning Rate 2.000e-05, It/sec 0.722, Tokens/sec 550.774, Trained Tokens 39886, Peak mem 2.708 GB\n",
      "Iter 56: Train loss 0.004, Learning Rate 1.667e-05, It/sec 0.778, Tokens/sec 533.756, Trained Tokens 40572, Peak mem 2.708 GB\n",
      "Iter 57: Train loss 0.004, Learning Rate 1.333e-05, It/sec 0.724, Tokens/sec 552.609, Trained Tokens 41335, Peak mem 2.708 GB\n",
      "Iter 58: Train loss 0.004, Learning Rate 1.000e-05, It/sec 0.786, Tokens/sec 539.327, Trained Tokens 42021, Peak mem 2.708 GB\n",
      "Iter 59: Train loss 0.004, Learning Rate 6.667e-06, It/sec 0.718, Tokens/sec 548.014, Trained Tokens 42784, Peak mem 2.708 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 2/2 [00:00<00:00,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 60: Val loss 0.004, Val took 0.941s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 60: Train loss 0.004, Learning Rate 3.333e-06, It/sec 0.778, Tokens/sec 533.714, Trained Tokens 43470, Peak mem 2.708 GB\n",
      "Saved final weights to outputs/adapters/adapters.safetensors.\n",
      "  Adapter config saved to: outputs/adapters/adapter_config.json\n",
      "\n",
      "======================================================================\n",
      "Training Complete!\n",
      "======================================================================\n",
      "  Adapters saved to: outputs/adapters\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b13cff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference mode enabled with KV caching\n",
      "Here's a simple Frends process definition that creates a simple flow:\n",
      "\n",
      "```json\n",
      "{\n",
      " \"Processes\": [{\n",
      " \"Name\": \"FirstProcess\",\n",
      " \"Version\": 1,\n",
      " \"ElementParameters\": [\n",
      " {\n",
      " \"Id\": \"StartEvent_1\",\n",
      " \"Type\": 0,\n",
      " \"SelectedTypeId\": \"ManualTrigger\",\n",
      " \"Name\": \"Manual Trigger\"\n",
      " },\n",
      " {\n",
      " \"Id\": \"Activity_0zi7vwm\",\n",
      " \"Type\": 12,\n",
      " \"Parameters\": {\n",
      " \"variableName\": \"result\",\n",
      " \"variableExpression\": {\n",
      " \"mode\": \"text\",\n",
      " \"value\": \"ok\"\n",
      " },\n",
      " \"shouldAssignVariable\": {\n",
      " \"mode\": \"toggle\",\n",
      " \"value\": true\n",
      " }\n",
      " },\n",
      " \"Name\": \"Assign result\"\n",
      " },\n",
      " {\n",
      " \"Id\": \"Event_19glogw\",\n",
      " \"Type\": 5,\n",
      " \"Parameters\": {\n",
      " \"expression\": {\n",
      " \"mode\": \"csharp\",\n",
      " \"value\": \"#var.result\"\n",
      " }\n",
      " }\n",
      " }\n",
      " ],\n",
      " \"TriggersJson\": \"[{\\\"$type\\\":\\\"ManualTrigger\\\",\\\"name\\\":\\\"Manual Trigger\\\",\\\"id\\\":\\\"StartEvent_1\\\"}]\"\n",
      " }]\n",
      "}```\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import generate\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "prompt = \"Generate sample Frends BPMN process. Give it a name FirstProcess. Output in JSON only.\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "response = generate(\n",
    "    model.model, tokenizer,\n",
    "    prompt=formatted_prompt,\n",
    "    max_tokens=10000,\n",
    "    verbose=False,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc81c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frends-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
