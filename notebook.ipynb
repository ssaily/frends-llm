{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cec2d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c79cbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "135d4fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.2.1: Fast Llama patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    NVIDIA GeForce GTX 1650. Num GPUs = 1. Max memory: 3.627 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.34. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    "    dtype=torch.float16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42655181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2026.2.1 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acd14d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.2\",\n",
    ")\n",
    "\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d72fbe72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212c58ccf0104972a615b4ed59a90ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"datasets/frends-bpmn-dataset.jsonl\", split=\"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8a2fcab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Create a Frends process that assigns a variable and returns it as output.',\n",
       " 'input': '',\n",
       " 'output': 'Here\\'s a simple Frends process definition that assigns a variable and returns it:\\n\\n```json\\n{\\n \"Processes\": [{\\n \"Name\": \"Fine Tuning: simple\",\\n \"Version\": 1,\\n \"ElementParameters\": [\\n {\\n \"Id\": \"StartEvent_1\",\\n \"Type\": 0,\\n \"SelectedTypeId\": \"ManualTrigger\",\\n \"Name\": \"Manual Trigger\"\\n },\\n {\\n \"Id\": \"Activity_0zi7vwm\",\\n \"Type\": 12,\\n \"Parameters\": {\\n \"variableName\": \"result\",\\n \"variableExpression\": {\\n \"mode\": \"text\",\\n \"value\": \"ok\"\\n },\\n \"shouldAssignVariable\": {\\n \"mode\": \"toggle\",\\n \"value\": true\\n }\\n },\\n \"Name\": \"Assign result\"\\n },\\n {\\n \"Id\": \"Event_19glogw\",\\n \"Type\": 5,\\n \"Parameters\": {\\n \"expression\": {\\n \"mode\": \"csharp\",\\n \"value\": \"#var.result\"\\n }\\n }\\n }\\n ],\\n \"TriggersJson\": \"[{\\\\\"$type\\\\\":\\\\\"ManualTrigger\\\\\",\\\\\"name\\\\\":\\\\\"Manual Trigger\\\\\",\\\\\"id\\\\\":\\\\\"StartEvent_1\\\\\"}]\"\\n }]\\n}```\\n\\nThis process uses a Manual Trigger to start, assigns the variable \\'result\\' with value \\'ok\\', and returns it at the end event.',\n",
       " 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCreate a Frends process that assigns a variable and returns it as output.\\n\\n### Input:\\n\\n\\n### Response:\\nHere\\'s a simple Frends process definition that assigns a variable and returns it:\\n\\n```json\\n{\\n \"Processes\": [{\\n \"Name\": \"Fine Tuning: simple\",\\n \"Version\": 1,\\n \"ElementParameters\": [\\n {\\n \"Id\": \"StartEvent_1\",\\n \"Type\": 0,\\n \"SelectedTypeId\": \"ManualTrigger\",\\n \"Name\": \"Manual Trigger\"\\n },\\n {\\n \"Id\": \"Activity_0zi7vwm\",\\n \"Type\": 12,\\n \"Parameters\": {\\n \"variableName\": \"result\",\\n \"variableExpression\": {\\n \"mode\": \"text\",\\n \"value\": \"ok\"\\n },\\n \"shouldAssignVariable\": {\\n \"mode\": \"toggle\",\\n \"value\": true\\n }\\n },\\n \"Name\": \"Assign result\"\\n },\\n {\\n \"Id\": \"Event_19glogw\",\\n \"Type\": 5,\\n \"Parameters\": {\\n \"expression\": {\\n \"mode\": \"csharp\",\\n \"value\": \"#var.result\"\\n }\\n }\\n }\\n ],\\n \"TriggersJson\": \"[{\\\\\"$type\\\\\":\\\\\"ManualTrigger\\\\\",\\\\\"name\\\\\":\\\\\"Manual Trigger\\\\\",\\\\\"id\\\\\":\\\\\"StartEvent_1\\\\\"}]\"\\n }]\\n}```\\n\\nThis process uses a Manual Trigger to start, assigns the variable \\'result\\' with value \\'ok\\', and returns it at the end event.<|eot_id|>'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f3039e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 4. Reducing num_proc to 4 for dataset of size 4.\n",
      "[datasets.arrow_dataset|WARNING]num_proc must be <= 4. Reducing num_proc to 4 for dataset of size 4.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e5c4acc1f24715bbc61f78ce006d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 2048,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f15332cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce GTX 1650. Max memory = 3.627 GB.\n",
      "2.176 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6ff141e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 4 | Num Epochs = 60 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 11,272,192 of 1,247,086,592 (0.90% trained)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 234.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 203.75 MiB is free. Process 3791052 has 3.42 GiB memory in use. Of the allocated memory 3.31 GiB is allocated by PyTorch, and 19.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trainer_stats = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/unsloth_compiled_cache/UnslothSFTTrainer.py:66\u001b[39m, in \u001b[36mprepare_for_training_mode.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mfor_training\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.for_training(use_gradient_checkpointing=use_gc)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m output = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# Restore previous mode when possible\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mfor_inference\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:330\u001b[39m, in \u001b[36m_fast_inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/unsloth_compiled_cache/UnslothSFTTrainer.py:1355\u001b[39m, in \u001b[36m_UnslothSFTTrainer.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1353\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1354\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maybe_activation_offload_context:\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:40\u001b[39m, in \u001b[36m_unsloth_training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/unsloth_compiled_cache/UnslothSFTTrainer.py:1344\u001b[39m, in \u001b[36m_UnslothSFTTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   1341\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_loss\u001b[39m(\n\u001b[32m   1342\u001b[39m     \u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs = \u001b[38;5;28;01mFalse\u001b[39;00m, num_items_in_batch = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1343\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1344\u001b[39m     outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1347\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1348\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1349\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/unsloth/models/_utils.py:1830\u001b[39m, in \u001b[36m_unsloth_pre_compute_loss\u001b[39m\u001b[34m(self, model, inputs, *args, **kwargs)\u001b[39m\n\u001b[32m   1827\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _has_ccm \u001b[38;5;129;01mand\u001b[39;00m _inner.training:\n\u001b[32m   1828\u001b[39m             inputs[\u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m] = torch.zeros_like(inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m1830\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_old_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1831\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:36\u001b[39m, in \u001b[36mcompute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/accelerate/utils/operations.py:819\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m819\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/accelerate/utils/operations.py:807\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m807\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/accelerate/utils/operations.py:819\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m819\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/accelerate/utils/operations.py:807\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m807\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/torch/_compile.py:54\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     51\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive, wrapping=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     52\u001b[39m     fn.__dynamo_disable = disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:1181\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1171\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m fx_traceback.annotate(\n\u001b[32m   1172\u001b[39m             {\n\u001b[32m   1173\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m_torchdynamo_disable\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1178\u001b[39m             }\n\u001b[32m   1179\u001b[39m         ):\n\u001b[32m   1180\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1183\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/unsloth/models/llama.py:1576\u001b[39m, in \u001b[36mPeftModel_fast_forward\u001b[39m\u001b[34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, num_logits_to_keep, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m   1565\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.base_model(\n\u001b[32m   1566\u001b[39m         input_ids = input_ids,\n\u001b[32m   1567\u001b[39m         attention_mask = attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1573\u001b[39m         **kwargs,\n\u001b[32m   1574\u001b[39m     )\n\u001b[32m   1575\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1576\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1577\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1578\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1579\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1580\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1581\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1582\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1583\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1584\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1585\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1587\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1588\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/peft/tuners/tuners_utils.py:311\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/unsloth/models/llama.py:1452\u001b[39m, in \u001b[36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[39m\u001b[34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, logits_to_keep, *args, **kwargs)\u001b[39m\n\u001b[32m   1441\u001b[39m     hidden_states = hidden_states * \u001b[38;5;28mself\u001b[39m.config.lm_head_multiplier\n\u001b[32m   1443\u001b[39m \u001b[38;5;66;03m### DISABLED since T4 breaks\u001b[39;00m\n\u001b[32m   1444\u001b[39m \u001b[38;5;66;03m# OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.\u001b[39;00m\n\u001b[32m   1445\u001b[39m \u001b[38;5;66;03m# loss = fused_linear_cross_entropy(\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1450\u001b[39m \u001b[38;5;66;03m#     logit_softcapping  = logit_softcapping,\u001b[39;00m\n\u001b[32m   1451\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1452\u001b[39m loss = \u001b[43munsloth_fused_ce_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1453\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1454\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1455\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlm_head_weight\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_head\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1456\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlm_head_bias\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1457\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1458\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1459\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_items\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maccelerator_scaler\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_gb\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1462\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_compile\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogit_softcapping\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_softcapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[32m   1466\u001b[39m     output = (logits,) + outputs[\u001b[32m1\u001b[39m:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/unsloth_zoo/fused_losses/cross_entropy_loss.py:376\u001b[39m, in \u001b[36munsloth_fused_ce_loss\u001b[39m\u001b[34m(trainer, hidden_states, lm_head_weight, lm_head_bias, labels, mask, n_items, scaling, target_gb, torch_compile, overwrite, **kwargs)\u001b[39m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TARGET_GB: target_gb = \u001b[38;5;28mfloat\u001b[39m(TARGET_GB)\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m N_CHUNKS: kwargs[\u001b[33m\"\u001b[39m\u001b[33mn_chunks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mint\u001b[39m(N_CHUNKS), \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_autograd_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mUnslothFusedLoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_fused_ce_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlm_head_weight\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_head_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlm_head_bias\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_head_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_items\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_gb\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_gb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_compile\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_compile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/unsloth_zoo/fused_losses/cross_entropy_loss.py:46\u001b[39m, in \u001b[36mapply_autograd_function\u001b[39m\u001b[34m(autograd, mapping)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_autograd_function\u001b[39m(autograd, mapping):\n\u001b[32m     45\u001b[39m     parameters, defaults = _get_mapping(autograd)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapply\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mold_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/torch/autograd/function.py:583\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    581\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    582\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m583\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    586\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    587\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    588\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    589\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    590\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    591\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/unsloth_zoo/fused_losses/cross_entropy_loss.py:314\u001b[39m, in \u001b[36mUnslothFusedLoss.forward\u001b[39m\u001b[34m(ctx, loss_function, hidden_states, lm_head_weight, lm_head_bias, labels, mask, n_items, scaling, shift_labels, target_gb, torch_compile, overwrite, extra_kwargs)\u001b[39m\n\u001b[32m    305\u001b[39m     accumulate_chunk = torch.compile(\n\u001b[32m    306\u001b[39m         accumulate_chunk,\n\u001b[32m    307\u001b[39m         dynamic = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    308\u001b[39m         fullgraph = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    309\u001b[39m         options = torch_compile_options,\n\u001b[32m    310\u001b[39m     )\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (grad_inputs_j, hidden_states_j, labels_j,) \u001b[38;5;129;01min\u001b[39;00m \\\n\u001b[32m    313\u001b[39m     \u001b[38;5;28mzip\u001b[39m(__grad_inputs, __shift_states, __shift_labels,):\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     \u001b[43maccumulate_chunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_chunks\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_inputs_j\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_inputs_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_lm_head\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_lm_head\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_lm_head_bias\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_lm_head_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states_j\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlm_head_weight\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_head_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlm_head_bias\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_head_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels_j\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdivisor\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdivisor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    329\u001b[39m ctx.save_for_backward(grad_inputs, grad_lm_head, grad_lm_head_bias)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:953\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    950\u001b[39m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[32m    952\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    955\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/unsloth_zoo/fused_losses/cross_entropy_loss.py:217\u001b[39m, in \u001b[36mUnslothFusedLoss.forward.<locals>.accumulate_chunk\u001b[39m\u001b[34m(n_chunks, grad_inputs_j, grad_lm_head, grad_lm_head_bias, hidden_states_j, lm_head_weight, lm_head_bias, labels_j, divisor, scaling, shift_labels, **kwargs)\u001b[39m\n\u001b[32m    214\u001b[39m __shift_states = torch.chunk(hidden_states.view(-\u001b[32m1\u001b[39m, hd), n_chunks, dim = \u001b[32m0\u001b[39m)\n\u001b[32m    215\u001b[39m __grad_inputs  = torch.chunk(grad_inputs.view(-\u001b[32m1\u001b[39m, hd),   n_chunks, dim = \u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34maccumulate_chunk\u001b[39m(\n\u001b[32m    218\u001b[39m     n_chunks,\n\u001b[32m    219\u001b[39m     grad_inputs_j,\n\u001b[32m    220\u001b[39m     grad_lm_head,\n\u001b[32m    221\u001b[39m     grad_lm_head_bias,\n\u001b[32m    222\u001b[39m     hidden_states_j,\n\u001b[32m    223\u001b[39m     lm_head_weight,\n\u001b[32m    224\u001b[39m     lm_head_bias,\n\u001b[32m    225\u001b[39m     labels_j,\n\u001b[32m    226\u001b[39m     divisor = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    227\u001b[39m     scaling = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    228\u001b[39m     shift_labels = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    229\u001b[39m     **kwargs,\n\u001b[32m    230\u001b[39m ):\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m lm_head_requires_grad \u001b[38;5;129;01mand\u001b[39;00m lm_head_bias_requires_grad:\n\u001b[32m    232\u001b[39m         (chunk_grad_input, chunk_grad_lm_head, chunk_grad_lm_head_bias,), \\\n\u001b[32m    233\u001b[39m         (chunk_loss, (unscaled_loss,)) = \\\n\u001b[32m    234\u001b[39m         torch.func.grad_and_value(\n\u001b[32m   (...)\u001b[39m\u001b[32m    246\u001b[39m             **kwargs,\n\u001b[32m    247\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:1181\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1171\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m fx_traceback.annotate(\n\u001b[32m   1172\u001b[39m             {\n\u001b[32m   1173\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m_torchdynamo_disable\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1178\u001b[39m             }\n\u001b[32m   1179\u001b[39m         ):\n\u001b[32m   1180\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1183\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/torch/_functorch/aot_autograd.py:1148\u001b[39m, in \u001b[36maot_module_simplified.<locals>.forward\u001b[39m\u001b[34m(*runtime_args)\u001b[39m\n\u001b[32m   1146\u001b[39m full_args.extend(params_buffers_flat)\n\u001b[32m   1147\u001b[39m full_args.extend(runtime_args)\n\u001b[32m-> \u001b[39m\u001b[32m1148\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:357\u001b[39m, in \u001b[36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    355\u001b[39m         torch._C._set_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    356\u001b[39m     record_runtime_wrapper_prologue_exit(cm)\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m     all_outs = \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompiled_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m grad_enabled:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/utils.py:134\u001b[39m, in \u001b[36mcall_func_at_runtime_with_args\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m_boxed_call\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m         out = normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    136\u001b[39m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    137\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[32m    138\u001b[39m         warnings.warn(\n\u001b[32m    139\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt take boxed arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    140\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    141\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    142\u001b[39m             stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    143\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:1962\u001b[39m, in \u001b[36mSerializableCompiledFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1961\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m1962\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:531\u001b[39m, in \u001b[36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001b[39m\u001b[34m(runtime_args)\u001b[39m\n\u001b[32m    524\u001b[39m     out = \u001b[38;5;28mself\u001b[39m._functionalized_rng_runtime_epilogue(\n\u001b[32m    525\u001b[39m         runtime_metadata,\n\u001b[32m    526\u001b[39m         out,\n\u001b[32m    527\u001b[39m         \u001b[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001b[39;00m\n\u001b[32m    528\u001b[39m         runtime_metadata.num_forward_returns,\n\u001b[32m    529\u001b[39m     )\n\u001b[32m    530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruntime_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:729\u001b[39m, in \u001b[36mEffectTokensWrapper.post_compile.<locals>.inner_fn\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    726\u001b[39m     args = [*([\u001b[38;5;28;01mNone\u001b[39;00m] * num_tokens), *args]\n\u001b[32m    727\u001b[39m     old_args.clear()\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m outs = \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[38;5;66;03m# Inductor cache DummyModule can return None\u001b[39;00m\n\u001b[32m    732\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m outs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/torch/_inductor/output_code.py:638\u001b[39m, in \u001b[36mCompiledFxGraph.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    636\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_callable(inputs)\n\u001b[32m    637\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    640\u001b[39m     get_runtime_metrics_context().finish()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/torch/_inductor/utils.py:3220\u001b[39m, in \u001b[36malign_inputs_from_check_idxs.<locals>.run\u001b[39m\u001b[34m(new_inputs)\u001b[39m\n\u001b[32m   3216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(new_inputs: \u001b[38;5;28mlist\u001b[39m[InputType]) -> Any:\n\u001b[32m   3217\u001b[39m     old_tensors, new_tensors = copy_misaligned_inputs(\n\u001b[32m   3218\u001b[39m         new_inputs, inputs_to_check, mutated_input_idxs\n\u001b[32m   3219\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3220\u001b[39m     out = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3222\u001b[39m     \u001b[38;5;66;03m# If a mutated tensor was cloned to be aligned, we need to reflect back the mutation to the\u001b[39;00m\n\u001b[32m   3223\u001b[39m     \u001b[38;5;66;03m# original tensor.\u001b[39;00m\n\u001b[32m   3224\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(old_tensors):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/torchinductor_jovyan/el/celz3hyskewhffb6aleow6ornpbtmowrdugtbxpy4nckjf5veods.py:353\u001b[39m, in \u001b[36mRunner.call\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    351\u001b[39m buf1 = empty_strided_cuda((s31, \u001b[32m1\u001b[39m), (\u001b[32m1\u001b[39m, s31), torch.float32)\n\u001b[32m    352\u001b[39m buf2 = empty_strided_cuda((s31, \u001b[32m1\u001b[39m), (\u001b[32m1\u001b[39m, s31), torch.float32)\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m buf4 = \u001b[43mempty_strided_cuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms31\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m128256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m128256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [float_1, loss, _autograd_grad], Original ATen: [aten._to_copy, prims.prepare_softmax_online, aten.sub, aten._log_softmax, aten._log_softmax_backward_data, aten.nll_loss_backward, aten.expand, aten.arange, aten.view, aten.eq, aten.scalar_tensor, aten.where, aten.mul, aten.div]\u001b[39;00m\n\u001b[32m    355\u001b[39m stream0 = get_raw_stream(\u001b[32m0\u001b[39m)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 234.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 203.75 MiB is free. Process 3791052 has 3.42 GiB memory in use. Of the allocated memory 3.31 GiB is allocated by PyTorch, and 19.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b13cff9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
