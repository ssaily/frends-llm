{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c79cbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sami.saily/work/frends-llm/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from unsloth_mlx import (\n",
    "    FastLanguageModel,\n",
    "    SFTTrainer,\n",
    "    TrainingArguments,    \n",
    "    prepare_dataset,    \n",
    "    get_chat_template,\n",
    "    format_chat_template,\n",
    "    create_training_data,\n",
    "    save_model_hf_format,\n",
    "    export_to_gguf,\n",
    "    get_training_config,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "135d4fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 136031.48it/s]\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 7096\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"mlx-community/Llama-3.2-3B-Instruct-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42655181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA configuration set: rank=16, alpha=16, modules=['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj'], dropout=0\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\",\n",
    "                      \"mlp.gate_proj\", \"mlp.up_proj\", \"mlp.down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "841524be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): QuantizedEmbedding(128256, 3072, group_size=64, bits=4, mode=affine)\n",
       "    (layers.0): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.1): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.2): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.3): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.4): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.5): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.6): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.7): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.8): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.9): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.10): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.11): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.12): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.13): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.14): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.15): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.16): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.17): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.18): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.19): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.20): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.21): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.22): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.23): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.24): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.25): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.26): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (layers.27): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (k_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (v_proj): QuantizedLinear(input_dims=3072, output_dims=1024, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (o_proj): QuantizedLinear(input_dims=3072, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (rope): Llama3RoPE()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (down_proj): QuantizedLinear(input_dims=8192, output_dims=3072, bias=False, group_size=64, bits=4, mode=affine)\n",
       "        (up_proj): QuantizedLinear(input_dims=3072, output_dims=8192, bias=False, group_size=64, bits=4, mode=affine)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(3072, eps=1e-05)\n",
       "    )\n",
       "    (norm): RMSNorm(3072, eps=1e-05)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acd14d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.2\",\n",
    ")\n",
    "\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72fbe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    Split,\n",
    ")\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"datasets/frends-bpmn-dataset-train.jsonl\", split=Split.TRAIN)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "dataset_val = load_dataset(\"json\", data_files=\"datasets/frends-bpmn-dataset-val.jsonl\")\n",
    "dataset_val = dataset_val.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8a2fcab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Create a Frends process that assigns a variable and returns it as output.',\n",
       " 'input': '',\n",
       " 'output': 'Here\\'s a simple Frends process definition that assigns a variable and returns it:\\n\\n```json\\n{\\n \"Processes\": [{\\n \"Name\": \"Fine Tuning: simple\",\\n \"Version\": 1,\\n \"ElementParameters\": [\\n {\\n \"Id\": \"StartEvent_1\",\\n \"Type\": 0,\\n \"SelectedTypeId\": \"ManualTrigger\",\\n \"Name\": \"Manual Trigger\"\\n },\\n {\\n \"Id\": \"Activity_0zi7vwm\",\\n \"Type\": 12,\\n \"Parameters\": {\\n \"variableName\": \"result\",\\n \"variableExpression\": {\\n \"mode\": \"text\",\\n \"value\": \"ok\"\\n },\\n \"shouldAssignVariable\": {\\n \"mode\": \"toggle\",\\n \"value\": true\\n }\\n },\\n \"Name\": \"Assign result\"\\n },\\n {\\n \"Id\": \"Event_19glogw\",\\n \"Type\": 5,\\n \"Parameters\": {\\n \"expression\": {\\n \"mode\": \"csharp\",\\n \"value\": \"#var.result\"\\n }\\n }\\n }\\n ],\\n \"TriggersJson\": \"[{\\\\\"$type\\\\\":\\\\\"ManualTrigger\\\\\",\\\\\"name\\\\\":\\\\\"Manual Trigger\\\\\",\\\\\"id\\\\\":\\\\\"StartEvent_1\\\\\"}]\"\\n }]\\n}```\\n\\nThis process uses a Manual Trigger to start, assigns the variable \\'result\\' with value \\'ok\\', and returns it at the end event.',\n",
       " 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCreate a Frends process that assigns a variable and returns it as output.\\n\\n### Input:\\n\\n\\n### Response:\\nHere\\'s a simple Frends process definition that assigns a variable and returns it:\\n\\n```json\\n{\\n \"Processes\": [{\\n \"Name\": \"Fine Tuning: simple\",\\n \"Version\": 1,\\n \"ElementParameters\": [\\n {\\n \"Id\": \"StartEvent_1\",\\n \"Type\": 0,\\n \"SelectedTypeId\": \"ManualTrigger\",\\n \"Name\": \"Manual Trigger\"\\n },\\n {\\n \"Id\": \"Activity_0zi7vwm\",\\n \"Type\": 12,\\n \"Parameters\": {\\n \"variableName\": \"result\",\\n \"variableExpression\": {\\n \"mode\": \"text\",\\n \"value\": \"ok\"\\n },\\n \"shouldAssignVariable\": {\\n \"mode\": \"toggle\",\\n \"value\": true\\n }\\n },\\n \"Name\": \"Assign result\"\\n },\\n {\\n \"Id\": \"Event_19glogw\",\\n \"Type\": 5,\\n \"Parameters\": {\\n \"expression\": {\\n \"mode\": \"csharp\",\\n \"value\": \"#var.result\"\\n }\\n }\\n }\\n ],\\n \"TriggersJson\": \"[{\\\\\"$type\\\\\":\\\\\"ManualTrigger\\\\\",\\\\\"name\\\\\":\\\\\"Manual Trigger\\\\\",\\\\\"id\\\\\":\\\\\"StartEvent_1\\\\\"}]\"\\n }]\\n}```\\n\\nThis process uses a Manual Trigger to start, assigns the variable \\'result\\' with value \\'ok\\', and returns it at the end event.<|eot_id|>'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f3039e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized:\n",
      "  Output dir: outputs\n",
      "  Adapter path: outputs/adapters\n",
      "  Learning rate: 0.0002\n",
      "  Iterations: 60\n",
      "  Batch size: 2\n",
      "  LoRA r=16, alpha=16\n",
      "  Native training: True\n",
      "  LR scheduler: linear\n",
      "  Grad checkpoint: False\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    val_dataset = dataset_val,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,        \n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6ff141e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mx.metal.device_info is deprecated and will be removed in a future version. Use mx.device_info instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Starting Fine-Tuning\n",
      "======================================================================\n",
      "\n",
      "[Using Native MLX Training]\n",
      "\n",
      "Applying LoRA adapters...\n",
      "Applying LoRA to 28 layers: {'rank': 16, 'scale': 1.0, 'dropout': 0, 'keys': ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']}\n",
      "✓ LoRA applied successfully to 28 layers\n",
      "  Trainable LoRA parameters: 392\n",
      "Preparing training data...\n",
      "  Detected format: text\n",
      "✓ Prepared 4 training samples\n",
      "  Saved to: outputs/train.jsonl\n",
      "✓ Created validation set (copied from train)\n",
      "\n",
      "Training configuration:\n",
      "  Iterations: 60\n",
      "  Batch size: 2\n",
      "  Learning rate: 0.0002\n",
      "  LR scheduler: linear\n",
      "  Grad checkpoint: True\n",
      "  Adapter file: outputs/adapters/adapters.safetensors\n",
      "\n",
      "Loaded 4 training samples, 4 validation samples\n",
      "Starting training loop...\n",
      "Starting training..., iters: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 2/2 [00:02<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: Val loss 2.389, Val took 2.590s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: Train loss 2.224, Learning Rate 2.000e-04, It/sec 0.279, Tokens/sec 196.437, Trained Tokens 704, Peak mem 3.678 GB\n",
      "Iter 2: Train loss 2.306, Learning Rate 1.967e-04, It/sec 0.283, Tokens/sec 221.669, Trained Tokens 1488, Peak mem 3.846 GB\n",
      "Iter 3: Train loss 1.448, Learning Rate 1.933e-04, It/sec 0.288, Tokens/sec 202.774, Trained Tokens 2192, Peak mem 3.846 GB\n",
      "Iter 4: Train loss 1.348, Learning Rate 1.900e-04, It/sec 0.289, Tokens/sec 226.799, Trained Tokens 2976, Peak mem 3.846 GB\n",
      "Iter 5: Train loss 0.841, Learning Rate 1.867e-04, It/sec 0.290, Tokens/sec 227.030, Trained Tokens 3760, Peak mem 3.846 GB\n",
      "Iter 6: Train loss 0.537, Learning Rate 1.833e-04, It/sec 0.290, Tokens/sec 203.825, Trained Tokens 4464, Peak mem 3.846 GB\n",
      "Iter 7: Train loss 0.257, Learning Rate 1.800e-04, It/sec 0.286, Tokens/sec 201.135, Trained Tokens 5168, Peak mem 3.846 GB\n",
      "Iter 8: Train loss 0.354, Learning Rate 1.767e-04, It/sec 0.287, Tokens/sec 224.932, Trained Tokens 5952, Peak mem 3.846 GB\n",
      "Iter 9: Train loss 0.181, Learning Rate 1.733e-04, It/sec 0.286, Tokens/sec 224.181, Trained Tokens 6736, Peak mem 3.846 GB\n",
      "Iter 10: Train loss 0.139, Learning Rate 1.700e-04, It/sec 0.287, Tokens/sec 201.954, Trained Tokens 7440, Peak mem 3.846 GB\n",
      "Iter 11: Train loss 0.147, Learning Rate 1.667e-04, It/sec 0.286, Tokens/sec 224.569, Trained Tokens 8224, Peak mem 3.846 GB\n",
      "Iter 12: Train loss 0.087, Learning Rate 1.633e-04, It/sec 0.286, Tokens/sec 201.586, Trained Tokens 8928, Peak mem 3.846 GB\n",
      "Iter 13: Train loss 0.073, Learning Rate 1.600e-04, It/sec 0.287, Tokens/sec 224.718, Trained Tokens 9712, Peak mem 3.846 GB\n",
      "Iter 14: Train loss 0.058, Learning Rate 1.567e-04, It/sec 0.286, Tokens/sec 201.295, Trained Tokens 10416, Peak mem 3.846 GB\n",
      "Iter 15: Train loss 0.031, Learning Rate 1.533e-04, It/sec 0.287, Tokens/sec 201.897, Trained Tokens 11120, Peak mem 3.846 GB\n",
      "Iter 16: Train loss 0.063, Learning Rate 1.500e-04, It/sec 0.286, Tokens/sec 224.278, Trained Tokens 11904, Peak mem 3.846 GB\n",
      "Iter 17: Train loss 0.030, Learning Rate 1.467e-04, It/sec 0.286, Tokens/sec 224.536, Trained Tokens 12688, Peak mem 3.846 GB\n",
      "Iter 18: Train loss 0.048, Learning Rate 1.433e-04, It/sec 0.286, Tokens/sec 201.506, Trained Tokens 13392, Peak mem 3.846 GB\n",
      "Iter 19: Train loss 0.010, Learning Rate 1.400e-04, It/sec 0.286, Tokens/sec 224.069, Trained Tokens 14176, Peak mem 3.846 GB\n",
      "Iter 20: Train loss 0.023, Learning Rate 1.367e-04, It/sec 0.286, Tokens/sec 201.345, Trained Tokens 14880, Peak mem 3.846 GB\n",
      "Iter 21: Train loss 0.006, Learning Rate 1.333e-04, It/sec 0.286, Tokens/sec 224.408, Trained Tokens 15664, Peak mem 3.846 GB\n",
      "Iter 22: Train loss 0.009, Learning Rate 1.300e-04, It/sec 0.286, Tokens/sec 201.378, Trained Tokens 16368, Peak mem 3.846 GB\n",
      "Iter 23: Train loss 0.006, Learning Rate 1.267e-04, It/sec 0.287, Tokens/sec 224.912, Trained Tokens 17152, Peak mem 3.846 GB\n",
      "Iter 24: Train loss 0.008, Learning Rate 1.233e-04, It/sec 0.287, Tokens/sec 201.846, Trained Tokens 17856, Peak mem 3.846 GB\n",
      "Iter 25: Train loss 0.006, Learning Rate 1.200e-04, It/sec 0.286, Tokens/sec 224.066, Trained Tokens 18640, Peak mem 3.846 GB\n",
      "Iter 26: Train loss 0.007, Learning Rate 1.167e-04, It/sec 0.286, Tokens/sec 201.246, Trained Tokens 19344, Peak mem 3.846 GB\n",
      "Iter 27: Train loss 0.005, Learning Rate 1.133e-04, It/sec 0.286, Tokens/sec 201.539, Trained Tokens 20048, Peak mem 3.846 GB\n",
      "Iter 28: Train loss 0.007, Learning Rate 1.100e-04, It/sec 0.287, Tokens/sec 224.620, Trained Tokens 20832, Peak mem 3.846 GB\n",
      "Iter 29: Train loss 0.007, Learning Rate 1.067e-04, It/sec 0.286, Tokens/sec 224.314, Trained Tokens 21616, Peak mem 3.846 GB\n",
      "Iter 30: Train loss 0.004, Learning Rate 1.033e-04, It/sec 0.286, Tokens/sec 201.307, Trained Tokens 22320, Peak mem 3.846 GB\n",
      "Iter 31: Train loss 0.005, Learning Rate 1.000e-04, It/sec 0.286, Tokens/sec 224.381, Trained Tokens 23104, Peak mem 3.846 GB\n",
      "Iter 32: Train loss 0.004, Learning Rate 9.667e-05, It/sec 0.287, Tokens/sec 201.901, Trained Tokens 23808, Peak mem 3.846 GB\n",
      "Iter 33: Train loss 0.004, Learning Rate 9.333e-05, It/sec 0.286, Tokens/sec 224.423, Trained Tokens 24592, Peak mem 3.846 GB\n",
      "Iter 34: Train loss 0.004, Learning Rate 9.000e-05, It/sec 0.286, Tokens/sec 201.012, Trained Tokens 25296, Peak mem 3.846 GB\n",
      "Iter 35: Train loss 0.004, Learning Rate 8.667e-05, It/sec 0.288, Tokens/sec 225.568, Trained Tokens 26080, Peak mem 3.846 GB\n",
      "Iter 36: Train loss 0.005, Learning Rate 8.333e-05, It/sec 0.288, Tokens/sec 202.530, Trained Tokens 26784, Peak mem 3.846 GB\n",
      "Iter 37: Train loss 0.003, Learning Rate 8.000e-05, It/sec 0.288, Tokens/sec 225.907, Trained Tokens 27568, Peak mem 3.846 GB\n",
      "Iter 38: Train loss 0.005, Learning Rate 7.667e-05, It/sec 0.286, Tokens/sec 201.313, Trained Tokens 28272, Peak mem 3.846 GB\n",
      "Iter 39: Train loss 0.003, Learning Rate 7.333e-05, It/sec 0.286, Tokens/sec 224.237, Trained Tokens 29056, Peak mem 3.846 GB\n",
      "Iter 40: Train loss 0.005, Learning Rate 7.000e-05, It/sec 0.287, Tokens/sec 201.831, Trained Tokens 29760, Peak mem 3.846 GB\n",
      "Iter 41: Train loss 0.004, Learning Rate 6.667e-05, It/sec 0.286, Tokens/sec 201.333, Trained Tokens 30464, Peak mem 3.846 GB\n",
      "Iter 42: Train loss 0.004, Learning Rate 6.333e-05, It/sec 0.286, Tokens/sec 224.125, Trained Tokens 31248, Peak mem 3.846 GB\n",
      "Iter 43: Train loss 0.004, Learning Rate 6.000e-05, It/sec 0.287, Tokens/sec 201.940, Trained Tokens 31952, Peak mem 3.846 GB\n",
      "Iter 44: Train loss 0.004, Learning Rate 5.667e-05, It/sec 0.285, Tokens/sec 223.091, Trained Tokens 32736, Peak mem 3.846 GB\n",
      "Iter 45: Train loss 0.004, Learning Rate 5.333e-05, It/sec 0.287, Tokens/sec 225.398, Trained Tokens 33520, Peak mem 3.846 GB\n",
      "Iter 46: Train loss 0.004, Learning Rate 5.000e-05, It/sec 0.288, Tokens/sec 202.809, Trained Tokens 34224, Peak mem 3.846 GB\n",
      "Iter 47: Train loss 0.004, Learning Rate 4.667e-05, It/sec 0.289, Tokens/sec 203.606, Trained Tokens 34928, Peak mem 3.846 GB\n",
      "Iter 48: Train loss 0.004, Learning Rate 4.333e-05, It/sec 0.286, Tokens/sec 224.402, Trained Tokens 35712, Peak mem 3.846 GB\n",
      "Iter 49: Train loss 0.004, Learning Rate 4.000e-05, It/sec 0.288, Tokens/sec 226.061, Trained Tokens 36496, Peak mem 3.846 GB\n",
      "Iter 50: Train loss 0.004, Learning Rate 3.667e-05, It/sec 0.287, Tokens/sec 202.385, Trained Tokens 37200, Peak mem 3.846 GB\n",
      "Iter 51: Train loss 0.004, Learning Rate 3.333e-05, It/sec 0.286, Tokens/sec 201.209, Trained Tokens 37904, Peak mem 3.846 GB\n",
      "Iter 52: Train loss 0.004, Learning Rate 3.000e-05, It/sec 0.287, Tokens/sec 224.743, Trained Tokens 38688, Peak mem 3.846 GB\n",
      "Iter 53: Train loss 0.004, Learning Rate 2.667e-05, It/sec 0.286, Tokens/sec 224.533, Trained Tokens 39472, Peak mem 3.846 GB\n",
      "Iter 54: Train loss 0.004, Learning Rate 2.333e-05, It/sec 0.287, Tokens/sec 201.908, Trained Tokens 40176, Peak mem 3.846 GB\n",
      "Iter 55: Train loss 0.004, Learning Rate 2.000e-05, It/sec 0.286, Tokens/sec 201.639, Trained Tokens 40880, Peak mem 3.846 GB\n",
      "Iter 56: Train loss 0.004, Learning Rate 1.667e-05, It/sec 0.287, Tokens/sec 225.005, Trained Tokens 41664, Peak mem 3.846 GB\n",
      "Iter 57: Train loss 0.004, Learning Rate 1.333e-05, It/sec 0.287, Tokens/sec 202.111, Trained Tokens 42368, Peak mem 3.846 GB\n",
      "Iter 58: Train loss 0.004, Learning Rate 1.000e-05, It/sec 0.286, Tokens/sec 224.351, Trained Tokens 43152, Peak mem 3.846 GB\n",
      "Iter 59: Train loss 0.004, Learning Rate 6.667e-06, It/sec 0.286, Tokens/sec 224.288, Trained Tokens 43936, Peak mem 3.846 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 2/2 [00:02<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 60: Val loss 0.004, Val took 2.460s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 60: Train loss 0.004, Learning Rate 3.333e-06, It/sec 0.290, Tokens/sec 204.197, Trained Tokens 44640, Peak mem 3.846 GB\n",
      "Saved final weights to outputs/adapters/adapters.safetensors.\n",
      "  Adapter config saved to: outputs/adapters/adapter_config.json\n",
      "\n",
      "======================================================================\n",
      "Training Complete!\n",
      "======================================================================\n",
      "  Adapters saved to: outputs/adapters\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b13cff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference mode enabled with KV caching\n",
      "Here's a simple Frends BPMN process definition that assigns a variable and returns it as output:\n",
      "\n",
      "```json\n",
      "{\n",
      " \"Processes\": [{\n",
      " \"Name\": \"FirstProcess\",\n",
      " \"Version\": 1,\n",
      " \"ElementParameters\": [\n",
      " {\n",
      " \"Id\": \"StartEvent_1\",\n",
      " \"Type\": 0,\n",
      " \"SelectedTypeId\": \"ManualTrigger\",\n",
      " \"Name\": \"Manual Trigger\"\n",
      " },\n",
      " {\n",
      " \"Id\": \"Activity_0zi7vwm\",\n",
      " \"Type\": 12,\n",
      " \"Parameters\": {\n",
      " \"variableName\": \"result\",\n",
      " \"variableExpression\": {\n",
      " \"mode\": \"text\",\n",
      " \"value\": \"ok\"\n",
      " },\n",
      " \"shouldAssignVariable\": {\n",
      " \"mode\": \"toggle\",\n",
      " \"value\": true\n",
      " }\n",
      " },\n",
      " \"Name\": \"Assign result\"\n",
      " },\n",
      " {\n",
      " \"Id\": \"Event_19glogw\",\n",
      " \"Type\": 5,\n",
      " \"Parameters\": {\n",
      " \"expression\": {\n",
      " \"mode\": \"csharp\",\n",
      " \"value\": \"#var.result\"\n",
      " }\n",
      " }\n",
      " }\n",
      " ],\n",
      " \"TriggersJson\": \"[{\\\"$type\\\":\\\"ManualTrigger\\\",\\\"name\\\":\\\"Manual Trigger\\\",\\\"id\\\":\\\"StartEvent_1\\\"}]\"\n",
      " }]\n",
      "}```\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import generate\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "prompt = \"Generate sample Frends BPMN process. Give it a name FirstProcess. Output in JSON only.\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "response = generate(\n",
    "    model.model, tokenizer,\n",
    "    prompt=formatted_prompt,\n",
    "    max_tokens=10000,\n",
    "    verbose=False,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc81c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frends-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
