{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c79cbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sami.saily/work/frends-llm/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from unsloth_mlx import (\n",
    "    FastLanguageModel,\n",
    "    SFTTrainer,\n",
    "    TrainingArguments,    \n",
    "    prepare_dataset,    \n",
    "    get_chat_template,\n",
    "    format_chat_template,\n",
    "    create_training_data,\n",
    "    save_model_hf_format,\n",
    "    export_to_gguf,\n",
    "    get_training_config,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "135d4fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 74017.13it/s]\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 7096\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42655181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA configuration set: rank=16, alpha=16, modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], dropout=0\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acd14d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.2\",\n",
    ")\n",
    "\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d72fbe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    Split,\n",
    ")\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"datasets/frends-bpmn-dataset.jsonl\", split=Split.TRAIN)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "dataset_val = load_dataset(\"json\", data_files=\"datasets/frends-bpmn-dataset-val.jsonl\")\n",
    "dataset_val = dataset_val.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8a2fcab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Create a Frends process that assigns a variable and returns it as output.',\n",
       " 'input': '',\n",
       " 'output': 'Here\\'s a simple Frends process definition that assigns a variable and returns it:\\n\\n```json\\n{\\n \"Processes\": [{\\n \"Name\": \"Fine Tuning: simple\",\\n \"Version\": 1,\\n \"ElementParameters\": [\\n {\\n \"Id\": \"StartEvent_1\",\\n \"Type\": 0,\\n \"SelectedTypeId\": \"ManualTrigger\",\\n \"Name\": \"Manual Trigger\"\\n },\\n {\\n \"Id\": \"Activity_0zi7vwm\",\\n \"Type\": 12,\\n \"Parameters\": {\\n \"variableName\": \"result\",\\n \"variableExpression\": {\\n \"mode\": \"text\",\\n \"value\": \"ok\"\\n },\\n \"shouldAssignVariable\": {\\n \"mode\": \"toggle\",\\n \"value\": true\\n }\\n },\\n \"Name\": \"Assign result\"\\n },\\n {\\n \"Id\": \"Event_19glogw\",\\n \"Type\": 5,\\n \"Parameters\": {\\n \"expression\": {\\n \"mode\": \"csharp\",\\n \"value\": \"#var.result\"\\n }\\n }\\n }\\n ],\\n \"TriggersJson\": \"[{\\\\\"$type\\\\\":\\\\\"ManualTrigger\\\\\",\\\\\"name\\\\\":\\\\\"Manual Trigger\\\\\",\\\\\"id\\\\\":\\\\\"StartEvent_1\\\\\"}]\"\\n }]\\n}```\\n\\nThis process uses a Manual Trigger to start, assigns the variable \\'result\\' with value \\'ok\\', and returns it at the end event.',\n",
       " 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCreate a Frends process that assigns a variable and returns it as output.\\n\\n### Input:\\n\\n\\n### Response:\\nHere\\'s a simple Frends process definition that assigns a variable and returns it:\\n\\n```json\\n{\\n \"Processes\": [{\\n \"Name\": \"Fine Tuning: simple\",\\n \"Version\": 1,\\n \"ElementParameters\": [\\n {\\n \"Id\": \"StartEvent_1\",\\n \"Type\": 0,\\n \"SelectedTypeId\": \"ManualTrigger\",\\n \"Name\": \"Manual Trigger\"\\n },\\n {\\n \"Id\": \"Activity_0zi7vwm\",\\n \"Type\": 12,\\n \"Parameters\": {\\n \"variableName\": \"result\",\\n \"variableExpression\": {\\n \"mode\": \"text\",\\n \"value\": \"ok\"\\n },\\n \"shouldAssignVariable\": {\\n \"mode\": \"toggle\",\\n \"value\": true\\n }\\n },\\n \"Name\": \"Assign result\"\\n },\\n {\\n \"Id\": \"Event_19glogw\",\\n \"Type\": 5,\\n \"Parameters\": {\\n \"expression\": {\\n \"mode\": \"csharp\",\\n \"value\": \"#var.result\"\\n }\\n }\\n }\\n ],\\n \"TriggersJson\": \"[{\\\\\"$type\\\\\":\\\\\"ManualTrigger\\\\\",\\\\\"name\\\\\":\\\\\"Manual Trigger\\\\\",\\\\\"id\\\\\":\\\\\"StartEvent_1\\\\\"}]\"\\n }]\\n}```\\n\\nThis process uses a Manual Trigger to start, assigns the variable \\'result\\' with value \\'ok\\', and returns it at the end event.<|eot_id|>'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f3039e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized:\n",
      "  Output dir: outputs\n",
      "  Adapter path: outputs/adapters\n",
      "  Learning rate: 0.0002\n",
      "  Iterations: 60\n",
      "  Batch size: 2\n",
      "  LoRA r=16, alpha=16\n",
      "  Native training: True\n",
      "  LR scheduler: linear\n",
      "  Grad checkpoint: False\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    val_dataset = dataset_val,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,        \n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6ff141e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mx.metal.device_info is deprecated and will be removed in a future version. Use mx.device_info instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Starting Fine-Tuning\n",
      "======================================================================\n",
      "\n",
      "[Using Native MLX Training]\n",
      "\n",
      "Applying LoRA adapters...\n",
      "Applying LoRA to 16 layers: {'rank': 16, 'scale': 1.0, 'dropout': 0, 'keys': ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']}\n",
      "✓ LoRA applied successfully to 16 layers\n",
      "  Trainable LoRA parameters: 224\n",
      "Preparing training data...\n",
      "  Detected format: text\n",
      "✓ Prepared 4 training samples\n",
      "  Saved to: outputs/train.jsonl\n",
      "✓ Created validation set (copied from train)\n",
      "\n",
      "Training configuration:\n",
      "  Iterations: 60\n",
      "  Batch size: 2\n",
      "  Learning rate: 0.0002\n",
      "  LR scheduler: linear\n",
      "  Grad checkpoint: True\n",
      "  Adapter file: outputs/adapters/adapters.safetensors\n",
      "\n",
      "Loaded 4 training samples, 4 validation samples\n",
      "Starting training loop...\n",
      "Starting training..., iters: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 2/2 [00:01<00:00,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: Val loss 2.640, Val took 1.027s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: Train loss 2.826, Learning Rate 2.000e-04, It/sec 0.701, Tokens/sec 549.460, Trained Tokens 784, Peak mem 2.509 GB\n",
      "Iter 2: Train loss 2.193, Learning Rate 1.967e-04, It/sec 0.703, Tokens/sec 494.919, Trained Tokens 1488, Peak mem 2.708 GB\n",
      "Iter 3: Train loss 1.695, Learning Rate 1.933e-04, It/sec 0.728, Tokens/sec 512.607, Trained Tokens 2192, Peak mem 2.708 GB\n",
      "Iter 4: Train loss 1.508, Learning Rate 1.900e-04, It/sec 0.727, Tokens/sec 569.606, Trained Tokens 2976, Peak mem 2.708 GB\n",
      "Iter 5: Train loss 0.997, Learning Rate 1.867e-04, It/sec 0.734, Tokens/sec 575.256, Trained Tokens 3760, Peak mem 2.708 GB\n",
      "Iter 6: Train loss 0.671, Learning Rate 1.833e-04, It/sec 0.738, Tokens/sec 519.407, Trained Tokens 4464, Peak mem 2.708 GB\n",
      "Iter 7: Train loss 0.489, Learning Rate 1.800e-04, It/sec 0.737, Tokens/sec 577.746, Trained Tokens 5248, Peak mem 2.708 GB\n",
      "Iter 8: Train loss 0.304, Learning Rate 1.767e-04, It/sec 0.737, Tokens/sec 518.603, Trained Tokens 5952, Peak mem 2.708 GB\n",
      "Iter 9: Train loss 0.142, Learning Rate 1.733e-04, It/sec 0.735, Tokens/sec 517.634, Trained Tokens 6656, Peak mem 2.708 GB\n",
      "Iter 10: Train loss 0.260, Learning Rate 1.700e-04, It/sec 0.733, Tokens/sec 574.917, Trained Tokens 7440, Peak mem 2.708 GB\n",
      "Iter 11: Train loss 0.076, Learning Rate 1.667e-04, It/sec 0.735, Tokens/sec 517.776, Trained Tokens 8144, Peak mem 2.708 GB\n",
      "Iter 12: Train loss 0.169, Learning Rate 1.633e-04, It/sec 0.739, Tokens/sec 579.291, Trained Tokens 8928, Peak mem 2.708 GB\n",
      "Iter 13: Train loss 0.085, Learning Rate 1.600e-04, It/sec 0.739, Tokens/sec 519.930, Trained Tokens 9632, Peak mem 2.708 GB\n",
      "Iter 14: Train loss 0.079, Learning Rate 1.567e-04, It/sec 0.736, Tokens/sec 577.272, Trained Tokens 10416, Peak mem 2.708 GB\n",
      "Iter 15: Train loss 0.060, Learning Rate 1.533e-04, It/sec 0.736, Tokens/sec 518.209, Trained Tokens 11120, Peak mem 2.708 GB\n",
      "Iter 16: Train loss 0.049, Learning Rate 1.500e-04, It/sec 0.738, Tokens/sec 578.757, Trained Tokens 11904, Peak mem 2.708 GB\n",
      "Iter 17: Train loss 0.036, Learning Rate 1.467e-04, It/sec 0.733, Tokens/sec 516.150, Trained Tokens 12608, Peak mem 2.708 GB\n",
      "Iter 18: Train loss 0.042, Learning Rate 1.433e-04, It/sec 0.739, Tokens/sec 579.435, Trained Tokens 13392, Peak mem 2.708 GB\n",
      "Iter 19: Train loss 0.032, Learning Rate 1.400e-04, It/sec 0.724, Tokens/sec 509.765, Trained Tokens 14096, Peak mem 2.708 GB\n",
      "Iter 20: Train loss 0.027, Learning Rate 1.367e-04, It/sec 0.738, Tokens/sec 578.834, Trained Tokens 14880, Peak mem 2.708 GB\n",
      "Iter 21: Train loss 0.024, Learning Rate 1.333e-04, It/sec 0.740, Tokens/sec 579.866, Trained Tokens 15664, Peak mem 2.708 GB\n",
      "Iter 22: Train loss 0.024, Learning Rate 1.300e-04, It/sec 0.738, Tokens/sec 519.269, Trained Tokens 16368, Peak mem 2.708 GB\n",
      "Iter 23: Train loss 0.020, Learning Rate 1.267e-04, It/sec 0.737, Tokens/sec 519.195, Trained Tokens 17072, Peak mem 2.708 GB\n",
      "Iter 24: Train loss 0.015, Learning Rate 1.233e-04, It/sec 0.736, Tokens/sec 577.195, Trained Tokens 17856, Peak mem 2.708 GB\n",
      "Iter 25: Train loss 0.011, Learning Rate 1.200e-04, It/sec 0.734, Tokens/sec 575.670, Trained Tokens 18640, Peak mem 2.708 GB\n",
      "Iter 26: Train loss 0.009, Learning Rate 1.167e-04, It/sec 0.736, Tokens/sec 518.475, Trained Tokens 19344, Peak mem 2.708 GB\n",
      "Iter 27: Train loss 0.005, Learning Rate 1.133e-04, It/sec 0.728, Tokens/sec 571.082, Trained Tokens 20128, Peak mem 2.708 GB\n",
      "Iter 28: Train loss 0.005, Learning Rate 1.100e-04, It/sec 0.718, Tokens/sec 505.682, Trained Tokens 20832, Peak mem 2.708 GB\n",
      "Iter 29: Train loss 0.005, Learning Rate 1.067e-04, It/sec 0.727, Tokens/sec 570.022, Trained Tokens 21616, Peak mem 2.708 GB\n",
      "Iter 30: Train loss 0.005, Learning Rate 1.033e-04, It/sec 0.733, Tokens/sec 515.913, Trained Tokens 22320, Peak mem 2.708 GB\n",
      "Iter 31: Train loss 0.005, Learning Rate 1.000e-04, It/sec 0.734, Tokens/sec 575.251, Trained Tokens 23104, Peak mem 2.708 GB\n",
      "Iter 32: Train loss 0.004, Learning Rate 9.667e-05, It/sec 0.733, Tokens/sec 516.291, Trained Tokens 23808, Peak mem 2.708 GB\n",
      "Iter 33: Train loss 0.004, Learning Rate 9.333e-05, It/sec 0.733, Tokens/sec 515.816, Trained Tokens 24512, Peak mem 2.708 GB\n",
      "Iter 34: Train loss 0.006, Learning Rate 9.000e-05, It/sec 0.724, Tokens/sec 567.393, Trained Tokens 25296, Peak mem 2.708 GB\n",
      "Iter 35: Train loss 0.006, Learning Rate 8.667e-05, It/sec 0.725, Tokens/sec 568.496, Trained Tokens 26080, Peak mem 2.708 GB\n",
      "Iter 36: Train loss 0.003, Learning Rate 8.333e-05, It/sec 0.727, Tokens/sec 511.925, Trained Tokens 26784, Peak mem 2.708 GB\n",
      "Iter 37: Train loss 0.005, Learning Rate 8.000e-05, It/sec 0.731, Tokens/sec 573.229, Trained Tokens 27568, Peak mem 2.708 GB\n",
      "Iter 38: Train loss 0.004, Learning Rate 7.667e-05, It/sec 0.720, Tokens/sec 506.845, Trained Tokens 28272, Peak mem 2.708 GB\n",
      "Iter 39: Train loss 0.004, Learning Rate 7.333e-05, It/sec 0.734, Tokens/sec 575.134, Trained Tokens 29056, Peak mem 2.708 GB\n",
      "Iter 40: Train loss 0.005, Learning Rate 7.000e-05, It/sec 0.730, Tokens/sec 514.166, Trained Tokens 29760, Peak mem 2.708 GB\n",
      "Iter 41: Train loss 0.003, Learning Rate 6.667e-05, It/sec 0.728, Tokens/sec 570.698, Trained Tokens 30544, Peak mem 2.708 GB\n",
      "Iter 42: Train loss 0.005, Learning Rate 6.333e-05, It/sec 0.738, Tokens/sec 519.901, Trained Tokens 31248, Peak mem 2.708 GB\n",
      "Iter 43: Train loss 0.005, Learning Rate 6.000e-05, It/sec 0.734, Tokens/sec 516.740, Trained Tokens 31952, Peak mem 2.708 GB\n",
      "Iter 44: Train loss 0.004, Learning Rate 5.667e-05, It/sec 0.730, Tokens/sec 572.054, Trained Tokens 32736, Peak mem 2.708 GB\n",
      "Iter 45: Train loss 0.004, Learning Rate 5.333e-05, It/sec 0.722, Tokens/sec 507.942, Trained Tokens 33440, Peak mem 2.708 GB\n",
      "Iter 46: Train loss 0.004, Learning Rate 5.000e-05, It/sec 0.728, Tokens/sec 570.677, Trained Tokens 34224, Peak mem 2.708 GB\n",
      "Iter 47: Train loss 0.004, Learning Rate 4.667e-05, It/sec 0.730, Tokens/sec 571.929, Trained Tokens 35008, Peak mem 2.708 GB\n",
      "Iter 48: Train loss 0.004, Learning Rate 4.333e-05, It/sec 0.731, Tokens/sec 514.806, Trained Tokens 35712, Peak mem 2.708 GB\n",
      "Iter 49: Train loss 0.004, Learning Rate 4.000e-05, It/sec 0.734, Tokens/sec 575.597, Trained Tokens 36496, Peak mem 2.708 GB\n",
      "Iter 50: Train loss 0.004, Learning Rate 3.667e-05, It/sec 0.728, Tokens/sec 512.210, Trained Tokens 37200, Peak mem 2.708 GB\n",
      "Iter 51: Train loss 0.004, Learning Rate 3.333e-05, It/sec 0.727, Tokens/sec 511.839, Trained Tokens 37904, Peak mem 2.708 GB\n",
      "Iter 52: Train loss 0.004, Learning Rate 3.000e-05, It/sec 0.721, Tokens/sec 565.107, Trained Tokens 38688, Peak mem 2.708 GB\n",
      "Iter 53: Train loss 0.004, Learning Rate 2.667e-05, It/sec 0.701, Tokens/sec 549.713, Trained Tokens 39472, Peak mem 2.708 GB\n",
      "Iter 54: Train loss 0.004, Learning Rate 2.333e-05, It/sec 0.723, Tokens/sec 508.786, Trained Tokens 40176, Peak mem 2.708 GB\n",
      "Iter 55: Train loss 0.004, Learning Rate 2.000e-05, It/sec 0.706, Tokens/sec 496.923, Trained Tokens 40880, Peak mem 2.708 GB\n",
      "Iter 56: Train loss 0.004, Learning Rate 1.667e-05, It/sec 0.681, Tokens/sec 533.705, Trained Tokens 41664, Peak mem 2.708 GB\n",
      "Iter 57: Train loss 0.004, Learning Rate 1.333e-05, It/sec 0.708, Tokens/sec 554.707, Trained Tokens 42448, Peak mem 2.708 GB\n",
      "Iter 58: Train loss 0.004, Learning Rate 1.000e-05, It/sec 0.727, Tokens/sec 511.524, Trained Tokens 43152, Peak mem 2.708 GB\n",
      "Iter 59: Train loss 0.004, Learning Rate 6.667e-06, It/sec 0.721, Tokens/sec 565.176, Trained Tokens 43936, Peak mem 2.708 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 2/2 [00:00<00:00,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 60: Val loss 0.004, Val took 0.976s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 60: Train loss 0.004, Learning Rate 3.333e-06, It/sec 0.719, Tokens/sec 506.288, Trained Tokens 44640, Peak mem 2.708 GB\n",
      "Saved final weights to outputs/adapters/adapters.safetensors.\n",
      "  Adapter config saved to: outputs/adapters/adapter_config.json\n",
      "\n",
      "======================================================================\n",
      "Training Complete!\n",
      "======================================================================\n",
      "  Adapters saved to: outputs/adapters\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b13cff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference mode enabled with KV caching\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import generate\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "prompt = \"Generate sample Frends BPMN process. Give it a name FirstProcess. Output in JSON only.\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "response = generate(\n",
    "    model.model, tokenizer,\n",
    "    prompt=formatted_prompt,\n",
    "    max_tokens=10000,\n",
    "    verbose=False,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc81c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frends-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
